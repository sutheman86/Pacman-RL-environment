{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Pacman-world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_env\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. test pacman-world environment\n",
    "* Without wrapper, without DQN. Pacman will move randomly\n",
    "> ***Note: DO NOT set `render_mode='human'`, or the kernel will crash (if you run this Notebook locally)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "env_name = \"gymnasium_env/PacmanGymEnv\"\n",
    "env = gym.make(env_name, speedup=5.0)\n",
    "env = env.unwrapped #減少限制\n",
    "\n",
    "print(\"environment:\", env_name)\n",
    "print(\"action space:\", env.action_space.n)\n",
    "# print(\"action:\", env.unwrapped.get_action_meanings())\n",
    "print(\"observation space:\", env.observation_space.shape)\n",
    "\n",
    "state = env.reset()\n",
    "step = 0\n",
    "total_reward=0\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, info = env.step(action)\n",
    "    step+=1\n",
    "    total_reward += reward\n",
    "    print('\\rStep: {:3d} | Reward: {:.3f} / {:.3f} | Action: {:.3f} | Info: {}'.format(step, reward, total_reward, action, info), end=\"\")\n",
    "    # if step%30==0:\n",
    "    #     plt.figure()\n",
    "    #     plt.imshow(obs)\n",
    "    #     plt.axis('off')  # 关闭坐标轴\n",
    "    # print(f\"state: {state[1]}\")\n",
    "    if terminated:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define Wrapper\n",
    "* `PacmanEnvWrapper` will contain important information for DQN to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PacmanEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, k, img_size=(84,84)):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.env = gym.make(env_name, speedup=4.0)\n",
    "        self.img_size = img_size\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(k, img_size[0], img_size[1]), dtype=np.float32)\n",
    "\n",
    "    def _preprocess(self, state, th=0.182):\n",
    "        # TODO(Lab-1): Image processing.\n",
    "        state = np.array(Image.fromarray(state).resize(self.img_size,Image.BILINEAR))\n",
    "        state = state.astype(np.float32).mean(2) / 255.\n",
    "        state[state > th] = 1.0\n",
    "        state[state <= th] = 0.0\n",
    "\n",
    "        return state\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # 確認是否返回了tuple，並提取圖像\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "        state = self._preprocess(state)\n",
    "        state = state[np.newaxis, ...].repeat(self.k, axis=0)  # 堆疊多幀\n",
    "        return state\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        state_next = []\n",
    "        info =[]\n",
    "        reward = 0\n",
    "        terminated = False\n",
    "        \n",
    "        for i in range(self.k):\n",
    "            if not terminated:\n",
    "                state_next_f, reward_f, terminated_f, info_f = self.env.step(action)\n",
    "                state_next_f = self._preprocess(state_next_f)\n",
    "                reward += reward_f\n",
    "                terminated = terminated_f\n",
    "                info.append(info_f)\n",
    "            state_next.append(state_next_f[np.newaxis, ...])\n",
    "        state_next = np.concatenate(state_next, 0)\n",
    "        return state_next, reward, terminated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize Wrapper and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Code\n",
    "env_pacman = PacmanEnvWrapper(env, k=4, img_size=(84,84))\n",
    "print(\"observation space:\", env_pacman.observation_space.shape)\n",
    "\n",
    "state = env_pacman.reset()\n",
    "action = env_pacman.action_space.sample()\n",
    "obs, reward, terminated, info = env_pacman.step(action)\n",
    "print(obs)\n",
    "plt.imshow(obs[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Test `PacmanEnvWrapper`\n",
    "* Without DQN, test if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# env_name = \"gymnasium_env/PacmanGymEnv\"\n",
    "# env = gym.make(env_name, speedup=4.0)\n",
    "# env = env.unwrapped #減少限制\n",
    "\n",
    "# print(\"environment:\", env_name)\n",
    "# print(\"action space:\", env.action_space.n)\n",
    "# print(\"action:\", env.unwrapped.get_action_meanings())\n",
    "# print(\"observation space:\", env.observation_space.shape)\n",
    "pacman_env = PacmanEnvWrapper(env, k=4, img_size=(84,84))\n",
    "state = pacman_env.reset()\n",
    "step = 0\n",
    "total_reward = 0\n",
    "while True:\n",
    "    action = pacman_env.action_space.sample()\n",
    "    obs, reward, terminated, info = pacman_env.step(action)\n",
    "    step+=1\n",
    "    total_reward += reward\n",
    "    print('\\rStep: {:3d} | Reward: {:.3f} / {:.3f} | Action: {:.3f} | Info: {}'.format(step, reward, total_reward, action, info), end=\"\")\n",
    "    \n",
    "    # if step%40==0:\n",
    "    #     # print(30)\n",
    "    #     plt.figure()\n",
    "    #     plt.imshow(obs[0],cmap=\"gray\")\n",
    "    #     plt.axis('off')  # 关闭坐标轴\n",
    "    # print(f\"state: {state[1]}\")\n",
    "    if terminated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Output to GIF\n",
    "* remember to run it, it'll be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.getcwd()\n",
    "def save_gif(img_buffer, fname, gif_path=os.path.join(project_root, \"GIF\")):\n",
    "    if not os.path.exists(gif_path):\n",
    "        os.makedirs(gif_path)\n",
    "    img_buffer[0].save(os.path.join(gif_path, fname), save_all=True, append_images=img_buffer[1:], duration=3, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Define QNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QNet(nn.Module):\n",
    "    # TODO(Lab-4): Q-Network architecture.\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(QNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions),\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x)\n",
    "        out = self.fc(conv_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Define DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork():\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions,\n",
    "        input_shape,\n",
    "        qnet,\n",
    "        device,\n",
    "        learning_rate=2e-4,\n",
    "        reward_decay=0.99,\n",
    "        replace_target_iter=1000,\n",
    "        memory_size=10000,\n",
    "        batch_size=32,\n",
    "    ):\n",
    "        # initialize parameters\n",
    "        self.n_actions = n_actions\n",
    "        self.input_shape = input_shape\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.learn_step_counter = 0\n",
    "        self.init_memory()\n",
    "\n",
    "        # Network\n",
    "        self.qnet_eval = qnet(self.input_shape, self.n_actions).to(self.device)\n",
    "        self.qnet_target = qnet(self.input_shape, self.n_actions).to(self.device)\n",
    "        self.qnet_target.eval()\n",
    "        self.optimizer = optim.RMSprop(self.qnet_eval.parameters(), lr=self.lr)\n",
    "\n",
    "    def choose_action(self, state, epsilon=0):\n",
    "        # 將狀態轉換為 FloatTensor 並增加 batch 維度\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        actions_value = self.qnet_eval.forward(state)\n",
    "        if np.random.uniform() > epsilon:  # greedy\n",
    "            action = torch.max(actions_value, 1)[1].data.cpu().numpy()[0]\n",
    "        else:  # random\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # 替换目标网络参数\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.qnet_target.load_state_dict(self.qnet_eval.state_dict())\n",
    "\n",
    "        # 随机采样经验池中的一个批次\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "\n",
    "        b_s = torch.FloatTensor(self.memory[\"s\"][sample_index]).to(self.device)\n",
    "        b_a = torch.LongTensor(self.memory[\"a\"][sample_index]).to(self.device)\n",
    "        b_r = torch.FloatTensor(self.memory[\"r\"][sample_index]).to(self.device)\n",
    "        b_s_ = torch.FloatTensor(self.memory[\"s_\"][sample_index]).to(self.device)\n",
    "        b_d = torch.FloatTensor(self.memory[\"done\"][sample_index]).to(self.device)\n",
    "\n",
    "        # DQN 和 DDQN 两种方式\n",
    "        q_curr_eval = self.qnet_eval(b_s).gather(1, b_a)\n",
    "        q_next_target = self.qnet_target(b_s_).detach()\n",
    "        q_next_eval = self.qnet_eval(b_s_).detach()\n",
    "        next_state_values = q_next_target.gather(1, q_next_eval.max(1)[1].unsqueeze(1))  # DDQN\n",
    "        q_curr_recur = b_r + (1 - b_d) * self.gamma * next_state_values\n",
    "\n",
    "        # 损失计算\n",
    "        self.loss = F.smooth_l1_loss(q_curr_eval, q_curr_recur)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        return self.loss.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "    def init_memory(self):\n",
    "        # 初始化经验池\n",
    "        self.memory = {\n",
    "            \"s\": np.zeros((self.memory_size, *self.input_shape)),\n",
    "            \"a\": np.zeros((self.memory_size, 1)),\n",
    "            \"r\": np.zeros((self.memory_size, 1)),\n",
    "            \"s_\": np.zeros((self.memory_size, *self.input_shape)),\n",
    "            \"done\": np.zeros((self.memory_size, 1)),\n",
    "        }\n",
    "\n",
    "    def store_transition(self, s, a, r, s_, d):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[\"s\"][index] = s\n",
    "        self.memory[\"a\"][index] = np.array(a).reshape(-1, 1)\n",
    "        self.memory[\"r\"][index] = np.array(r).reshape(-1, 1)\n",
    "        self.memory[\"s_\"][index] = s_\n",
    "        self.memory[\"done\"][index] = np.array(d).reshape(-1, 1)\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def save_load_model(self, op, path=\"save\", fname=\"qnet.pt\"):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        file_path = os.path.join(path, fname)\n",
    "\n",
    "        if op == \"save\":\n",
    "            # 保存模型狀態、優化器狀態、學習步驟和經驗池計數\n",
    "            checkpoint = {\n",
    "                'qnet_eval_state_dict': self.qnet_eval.state_dict(),\n",
    "                'qnet_target_state_dict': self.qnet_target.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'learn_step_counter': self.learn_step_counter,\n",
    "                'memory_counter': self.memory_counter,\n",
    "            }\n",
    "            torch.save(checkpoint, file_path)\n",
    "            print(f\"Model saved successfully at {file_path}\")\n",
    "\n",
    "        elif op == \"load\":\n",
    "            try:\n",
    "                # 加載模型狀態、優化器狀態、學習步驟和經驗池計數\n",
    "                checkpoint = torch.load(file_path, map_location=self.device)\n",
    "\n",
    "                # 檢查是否包含所有必需的鍵\n",
    "                required_keys = ['qnet_eval_state_dict', 'qnet_target_state_dict', 'optimizer_state_dict']\n",
    "                missing_keys = [key for key in required_keys if key not in checkpoint]\n",
    "\n",
    "                if missing_keys:\n",
    "                    raise KeyError(f\"Missing keys in checkpoint: {missing_keys}\")\n",
    "\n",
    "                # 加載各部分的狀態\n",
    "                self.qnet_eval.load_state_dict(checkpoint['qnet_eval_state_dict'])\n",
    "                self.qnet_target.load_state_dict(checkpoint['qnet_target_state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "                # 選擇性地加載學習計數\n",
    "                self.learn_step_counter = checkpoint.get('learn_step_counter', 0)\n",
    "                self.memory_counter = checkpoint.get('memory_counter', 0)\n",
    "\n",
    "                print(\"Model loaded successfully from\", file_path)\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(f\"No saved model found at {file_path}, starting fresh.\")\n",
    "            except KeyError as e:\n",
    "                print(f\"Error loading model: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Define Epsilon Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_compute(frame_id, epsilon_max=1, epsilon_min=0.05, epsilon_decay=100000):\n",
    "    return epsilon_min + (epsilon_max - epsilon_min) * np.exp(-frame_id / epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Graph of epsilon(0) to epsilon(400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Code\n",
    "frame_ids = np.array(range(400000))\n",
    "epsilons = epsilon_compute(frame_ids)\n",
    "plt.plot(epsilons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Initialize DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_frames = 4\n",
    "img_size = (84,84)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "agent = DeepQNetwork(\n",
    "        n_actions = env.action_space.n,\n",
    "        input_shape = [stack_frames, *img_size],\n",
    "        qnet = QNet,\n",
    "        device = device,\n",
    "        learning_rate = 2e-4,\n",
    "        reward_decay = 0.99,\n",
    "        replace_target_iter = 1000,\n",
    "        memory_size = 10000,\n",
    "        batch_size = 1024,)\n",
    "\n",
    "print(agent.qnet_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Define `Play()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium_env\n",
    "def play(env, agent, stack_frames, img_size):\n",
    "    # Reset environment.\n",
    "    state = env.reset()\n",
    "    img_buffer = [Image.fromarray(state[0]*255)]\n",
    "\n",
    "    # Initialize information.\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    # One episode.\n",
    "    while True:\n",
    "        # Select action.\n",
    "        action = agent.choose_action(state, 1)\n",
    "\n",
    "        # Get next stacked state.\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        if step % 2 == 0:\n",
    "            img_buffer.append(Image.fromarray(state_next[0]*255))\n",
    "\n",
    "        state = state_next.copy()\n",
    "        step += 1\n",
    "        total_reward += reward\n",
    "        print('\\rStep: {:3d} | Reward: {:.3f} / {:.3f} | Action: {:.3f} | Info: {}'.format(step, reward, total_reward, action, info[0]), end=\"\")\n",
    "\n",
    "        if done or step>200:\n",
    "            print()\n",
    "            break\n",
    "\n",
    "    return img_buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Test play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test play() function, *you don't need to run it if you did not change it*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_pacman = PacmanEnvWrapper(env, k=4, img_size=(84,84))\n",
    "img_buffer = play(env_pacman, agent, stack_frames, img_size)\n",
    "save_gif(img_buffer, fname=\"test0.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Define `train()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, stack_frames, img_size, save_path=\"save\", max_steps=1000000):\n",
    "    total_step = 0\n",
    "    episode = 0\n",
    "\n",
    "        # 嘗試加載模型和訓練狀態\n",
    "    try:\n",
    "        print(\"Loading model and training status...\")\n",
    "        status = agent.save_load_model(op=\"load\", path=save_path, fname=\"qnet.pt\")\n",
    "        total_step = status[\"learn_step_counter\"]\n",
    "        episode = status[\"memory_counter\"]\n",
    "        print(f\"Resuming training from total_step={total_step}, episode={episode}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No previous model found. Starting training from scratch.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key in checkpoint: {e}\")\n",
    "\n",
    "    while total_step <= max_steps:\n",
    "        # Reset environment.\n",
    "        state = env.reset()\n",
    "\n",
    "        # 如果 state 是 tuple，提取圖像\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "        # Initialize information.\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        loss = 0\n",
    "\n",
    "        # One episode.\n",
    "        while True:\n",
    "            # TODO(Lab-6): Select action.\n",
    "            epsilon = epsilon_compute(total_step)\n",
    "            action = agent.choose_action(state, epsilon)\n",
    "\n",
    "            # Get next observation.\n",
    "            obs, reward, terminated, info = env.step(action)\n",
    "\n",
    "            # 如果 obs 是 tuple，提取圖像\n",
    "            if isinstance(obs, tuple):\n",
    "                obs = obs[0]\n",
    "\n",
    "            # 判斷是否遊戲結束\n",
    "            done = terminated\n",
    "\n",
    "            # TODO(Lab-7): Train RL model.\n",
    "            # Store transition and learn.\n",
    "            agent.store_transition(state, action, reward, obs, done)\n",
    "            if total_step > 4 * agent.batch_size:\n",
    "                loss = agent.learn()\n",
    "\n",
    "            state = obs.copy()  # 更新狀態\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "            total_reward += reward\n",
    "\n",
    "            # 確保 loss 為浮點數以便於打印\n",
    "            if total_step % 2 == 0 or done:\n",
    "                print('\\rEpisode: {:3d} | Step: {:3d} / {:3d} | Reward: {:.3f} / {:.3f} | Loss: {:.3f} | Epsilon: {:.3f}'\\\n",
    "                    .format(episode, step, total_step, reward, total_reward, loss, epsilon), end=\"\")\n",
    "\n",
    "            if total_step % 1000 == 0:\n",
    "                print(\"\\nSave Model ...\")\n",
    "                agent.save_load_model(\n",
    "                    op=\"save\",\n",
    "                    path=save_path,\n",
    "                    fname=\"qnet.pt\"\n",
    "                )\n",
    "                print(\"Generate GIF ...\")\n",
    "                img_buffer = play(env, agent, stack_frames, img_size)\n",
    "                save_gif(img_buffer, \"train_\" + str(total_step).zfill(6) + \".gif\")\n",
    "                print(\"Done !!\")\n",
    "\n",
    "            if done or step > 2000:\n",
    "                episode += 1\n",
    "                print()\n",
    "                break\n",
    "\n",
    "        if total_step > max_steps:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 600step/min on Colab (with T4 GPU), 400step/min on RTX3070 laptop. Pretty slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_pacman = PacmanEnvWrapper(env, k=4, img_size=(84,84))\n",
    "train(env_pacman, agent, stack_frames, img_size, save_path=os.path.join(project_root, \"save\"), max_steps=600000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_load_model(op=\"load\", path=os.path.join(project_root, \"save\"), fname=\"qnet.pt\")\n",
    "env_pacman = PacmanEnvWrapper(env, k=4, img_size=(84,84))\n",
    "img_buffer = play(env_pacman, agent, stack_frames, img_size)\n",
    "save_gif(img_buffer, \"eval.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Codes\n",
    "* Would probably be useful later or really just junks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.\n",
    "* This is to test `PacmanGymEnv` could run by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import gymnasium_env\n",
    "# import gymnasium\n",
    "# import warnings\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # 設定無視窗模式\n",
    "# os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "# # 忽略 DeprecationWarning\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# # 初始化環境\n",
    "# env = gymnasium.make('gymnasium_env/PacmanGymEnv', speedup=4.0)\n",
    "# obs, info = env.reset()\n",
    "# env_unwrapped = env.unwrapped   \n",
    "\n",
    "# # 用來保存每一幀的圖像\n",
    "# frames = []\n",
    "\n",
    "# # 設定最多 1000 步\n",
    "# for step in range(1000):\n",
    "#     action = env.action_space.sample()  # 隨機取樣一個動作\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    \n",
    "#     # 確保返回值非空並且是 numpy array 格式\n",
    "#     if obs is not None and isinstance(obs, np.ndarray):\n",
    "#         img = Image.fromarray(obs)  # 轉換為 PIL 圖像格式\n",
    "#         frames.append(img)  # 添加幀到 frames 列表中\n",
    "#     else:\n",
    "#         print(\"Render did not return a valid image.\")\n",
    "    \n",
    "#     print(f\"Step: {step + 1}, Action: {action}, Reward: {reward}, Info: {info}, Done: {done}\")\n",
    "\n",
    "#     # 檢查是否回合結束\n",
    "#     if done:\n",
    "#         print(\"Episode finished!\")\n",
    "#         break\n",
    "\n",
    "# # 關閉環境\n",
    "# env.close()\n",
    "\n",
    "# # 保存為 GIF\n",
    "# output_path = \"../Gif/pacman_game.gif\"\n",
    "# frames[0].save(output_path, save_all=True, append_images=frames[1:], duration=100, loop=1)\n",
    "# print(f\"GIF saved as {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.\n",
    "* Old implementation of `play()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def play(env, agent, stack_frames, img_size):\n",
    "#     #os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "#     # Reset environment.\n",
    "#     state = env.reset()\n",
    "\n",
    "#     # 如果state是tuple，提取第一個元素作為圖像\n",
    "#     if isinstance(state, tuple):\n",
    "#         state = state[0]\n",
    "\n",
    "#     # state形狀應該是 (k, 84, 84)，所以不再提取單幀\n",
    "#     # 確保數據類型為uint8，並去除不必要的維度\n",
    "#     state = (state * 255).astype(np.uint8)\n",
    "\n",
    "#     # 初始化圖像緩衝區\n",
    "#     img_buffer = [Image.fromarray(state[0])]  # 顯示第一幀\n",
    "\n",
    "#     # Initialize information.\n",
    "#     step = 0\n",
    "#     total_reward = 0\n",
    "\n",
    "#     # One episode.\n",
    "#     while True:\n",
    "#         # Select action.\n",
    "#         action = agent.choose_action(state, 0)\n",
    "\n",
    "#         # Get next stacked state.\n",
    "#         state_next, reward, terminated, info = env.step(action)\n",
    "\n",
    "#         # 如果 state_next 是 tuple，提取圖像\n",
    "#         if isinstance(state_next, tuple):\n",
    "#             state_next = state_next[0]\n",
    "\n",
    "#         # 不再提取單幀，直接使用多幀數據\n",
    "#         state_next = (state_next * 255).astype(np.uint8)\n",
    "\n",
    "#         # 每兩步存儲一幀圖像\n",
    "#         if step % 2 == 0:\n",
    "#             img_buffer.append(Image.fromarray(state_next[0]))  # 顯示第一幀\n",
    "\n",
    "#         state = state_next.copy()  # 更新狀態\n",
    "#         step += 1\n",
    "#         total_reward += reward\n",
    "#         print('\\rStep: {:3d} | Reward: {:.3f} / {:.3f} | Action: {:.3f} | Info: {}'.format(step, reward, total_reward, action, info[0]), end=\"\")\n",
    "\n",
    "\n",
    "#         # 檢查遊戲是否結束或步數超過2000\n",
    "#         if terminated  or step > 400:\n",
    "#             print()\n",
    "#             break\n",
    "\n",
    "#     return img_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.2\n",
    "* Old implementation of learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.\n",
    "* This piece of code attempted to load model but failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 嘗試加載模型和訓練狀態\n",
    "    # try:\n",
    "    #     print(\"Loading model and training status...\")\n",
    "    #     status = agent.save_load_model(op=\"load\", path=save_path, fname=\"qnet.pt\")\n",
    "    #     total_step = status[\"learn_step_counter\"]\n",
    "    #     episode = status[\"memory_counter\"]\n",
    "    #     print(f\"Resuming training from total_step={total_step}, episode={episode}\")\n",
    "    # except FileNotFoundError:\n",
    "    #     print(\"No previous model found. Starting training from scratch.\")\n",
    "    # except KeyError as e:\n",
    "    #     print(f\"Missing key in checkpoint: {e}\")\n",
    "\n",
    "    # try:\n",
    "    #     print(\"Loading model and training status...\")\n",
    "    #     status = agent.save_load_model(op=\"load\", path=save_path, fname=\"qnet.pt\")\n",
    "    #     total_step = status.get(\"total_step\", 0)\n",
    "    #     episode = status.get(\"episode\", 0)\n",
    "    #     print(f\"Resuming training from total_step={total_step}, episode={episode}\")\n",
    "    # except FileNotFoundError:\n",
    "    #     print(\"No previous model found. Starting training from scratch.\")\n",
    "\n",
    "    # def save_load_model(self, op, path=\"save\", fname=\"qnet.pt\"):\n",
    "    #     import os\n",
    "    #     if not os.path.exists(path):\n",
    "    #         os.makedirs(path)\n",
    "    #     file_path = os.path.join(path, fname)\n",
    "\n",
    "    #     if op == \"save\":\n",
    "    #         # 保存模型狀態、優化器狀態、學習步驟和經驗池計數\n",
    "    #         checkpoint = {\n",
    "    #             'qnet_eval_state_dict': self.qnet_eval.state_dict(),\n",
    "    #             'qnet_target_state_dict': self.qnet_target.state_dict(),\n",
    "    #             'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "    #             'learn_step_counter': self.learn_step_counter,\n",
    "    #             'memory_counter': self.memory_counter,\n",
    "    #         }\n",
    "    #         torch.save(checkpoint, file_path)\n",
    "\n",
    "    #     elif op == \"load\":\n",
    "    #         # 加載模型狀態、優化器狀態、學習步驟和經驗池計數\n",
    "    #         checkpoint = torch.load(file_path, map_location=self.device)\n",
    "    #         self.qnet_eval.load_state_dict(checkpoint['qnet_eval_state_dict'])\n",
    "    #         self.qnet_target.load_state_dict(checkpoint['qnet_target_state_dict'])\n",
    "    #         self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    #         self.learn_step_counter = checkpoint.get('learn_step_counter', -1)\n",
    "    #         self.memory_counter = checkpoint.get('memory_counter', -1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pacman-RL-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
