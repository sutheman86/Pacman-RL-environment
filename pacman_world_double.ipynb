{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Pacman-world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_env\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Wrapper\n",
    "* `PacmanEnvWrapper` will contain important information for DQN to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PacmanEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, k, img_size=(84,84)):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.env = gym.make(env_name, speedup=5.0)\n",
    "        self.img_size = img_size\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(k, img_size[0], img_size[1]), dtype=np.float32)\n",
    "\n",
    "    def _preprocess(self, state, th=0.182):\n",
    "        # TODO(Lab-1): Image processing.\n",
    "        state = np.array(Image.fromarray(state).resize(self.img_size,Image.BILINEAR))\n",
    "        state = state.astype(np.float32).mean(2) / 255.\n",
    "        state[state > th] = 1.0\n",
    "        state[state <= th] = 0.0\n",
    "\n",
    "        return state\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # 確認是否返回了tuple，並提取圖像\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "        state = self._preprocess(state)\n",
    "        state = state[np.newaxis, ...].repeat(self.k, axis=0)  # 堆疊多幀\n",
    "        return state\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        state_next = []\n",
    "        info =[]\n",
    "        reward = 0\n",
    "        terminated = False\n",
    "        \n",
    "        for i in range(self.k):\n",
    "            if not terminated:\n",
    "                state_next_f, reward_f, terminated_f, info_f = self.env.step(action)\n",
    "                state_next_f = self._preprocess(state_next_f)\n",
    "                reward += reward_f\n",
    "                terminated = terminated_f\n",
    "                info.append(info_f)\n",
    "            state_next.append(state_next_f[np.newaxis, ...])\n",
    "        state_next = np.concatenate(state_next, 0)\n",
    "        return state_next, reward, terminated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Define Dueling QNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QNet(nn.Module):\n",
    "    # TODO(Lab-4): Q-Network architecture.\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(QNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions),\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x)\n",
    "        out = self.fc(conv_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Define DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork():\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions,\n",
    "        input_shape,\n",
    "        qnet,\n",
    "        device,\n",
    "        learning_rate=2e-4,\n",
    "        reward_decay=0.99,\n",
    "        replace_target_iter=1000,\n",
    "        memory_size=10000,\n",
    "        batch_size=32,\n",
    "    ):\n",
    "        # initialize parameters\n",
    "        self.n_actions = n_actions\n",
    "        self.input_shape = input_shape\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.learn_step_counter = 0\n",
    "        self.init_memory()\n",
    "\n",
    "        # Network\n",
    "        self.qnet_eval = qnet(self.input_shape, self.n_actions).to(self.device)\n",
    "        self.qnet_target = qnet(self.input_shape, self.n_actions).to(self.device)\n",
    "        self.qnet_target.eval()\n",
    "        self.optimizer = optim.RMSprop(self.qnet_eval.parameters(), lr=self.lr)\n",
    "\n",
    "        # Keep Trach of episodes\n",
    "        self.episode = 0\n",
    "\n",
    "    def choose_action(self, state, epsilon=0):\n",
    "        # 將狀態轉換為 FloatTensor 並增加 batch 維度\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        actions_value = self.qnet_eval.forward(state)\n",
    "        if np.random.uniform() > epsilon:  # greedy\n",
    "            action = torch.max(actions_value, 1)[1].data.cpu().numpy()[0]\n",
    "        else:  # random\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # 替换目标网络参数\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.qnet_target.load_state_dict(self.qnet_eval.state_dict())\n",
    "\n",
    "        # 随机采样经验池中的一个批次\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "\n",
    "        b_s = torch.FloatTensor(self.memory[\"s\"][sample_index]).to(self.device)\n",
    "        b_a = torch.LongTensor(self.memory[\"a\"][sample_index]).to(self.device)\n",
    "        b_r = torch.FloatTensor(self.memory[\"r\"][sample_index]).to(self.device)\n",
    "        b_s_ = torch.FloatTensor(self.memory[\"s_\"][sample_index]).to(self.device)\n",
    "        b_d = torch.FloatTensor(self.memory[\"done\"][sample_index]).to(self.device)\n",
    "\n",
    "        # DQN 和 DDQN 两种方式\n",
    "        q_curr_eval = self.qnet_eval(b_s).gather(1, b_a)\n",
    "        q_next_target = self.qnet_target(b_s_).detach()\n",
    "        q_next_eval = self.qnet_eval(b_s_).detach()\n",
    "        next_state_values = q_next_target.gather(1, q_next_eval.max(1)[1].unsqueeze(1))  # DDQN\n",
    "        q_curr_recur = b_r + (1 - b_d) * self.gamma * next_state_values\n",
    "\n",
    "        # 损失计算\n",
    "        self.loss = F.smooth_l1_loss(q_curr_eval, q_curr_recur)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        return self.loss.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "    def init_memory(self):\n",
    "        # 初始化经验池\n",
    "        self.memory = {\n",
    "            \"s\": np.zeros((self.memory_size, *self.input_shape)),\n",
    "            \"a\": np.zeros((self.memory_size, 1)),\n",
    "            \"r\": np.zeros((self.memory_size, 1)),\n",
    "            \"s_\": np.zeros((self.memory_size, *self.input_shape)),\n",
    "            \"done\": np.zeros((self.memory_size, 1)),\n",
    "        }\n",
    "\n",
    "    def store_transition(self, s, a, r, s_, d):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[\"s\"][index] = s\n",
    "        self.memory[\"a\"][index] = np.array(a).reshape(-1, 1)\n",
    "        self.memory[\"r\"][index] = np.array(r).reshape(-1, 1)\n",
    "        self.memory[\"s_\"][index] = s_\n",
    "        self.memory[\"done\"][index] = np.array(d).reshape(-1, 1)\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def save_load_model(self, op, path=\"save\", fname=\"qnet.pt\"):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        file_path = os.path.join(path, fname)\n",
    "\n",
    "        if op == \"save\":\n",
    "            # 保存模型狀態、優化器狀態、學習步驟和經驗池計數\n",
    "            checkpoint = {\n",
    "                'qnet_eval_state_dict': self.qnet_eval.state_dict(),\n",
    "                'qnet_target_state_dict': self.qnet_target.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'learn_step_counter': self.learn_step_counter,\n",
    "                'memory_counter': self.memory_counter,\n",
    "                'episode': self.episode,\n",
    "            }\n",
    "            torch.save(checkpoint, file_path)\n",
    "            print(f\"Model saved successfully at {file_path}\")\n",
    "\n",
    "        elif op == \"load\":\n",
    "            try:\n",
    "                # 加載模型狀態、優化器狀態、學習步驟和經驗池計數\n",
    "                checkpoint = torch.load(file_path, map_location=self.device)\n",
    "\n",
    "                # 檢查是否包含所有必需的鍵\n",
    "                required_keys = ['qnet_eval_state_dict', 'qnet_target_state_dict', 'optimizer_state_dict']\n",
    "                missing_keys = [key for key in required_keys if key not in checkpoint]\n",
    "\n",
    "                if missing_keys:\n",
    "                    raise KeyError(f\"Missing keys in checkpoint: {missing_keys}\")\n",
    "\n",
    "                # 加載各部分的狀態\n",
    "                self.qnet_eval.load_state_dict(checkpoint['qnet_eval_state_dict'])\n",
    "                self.qnet_target.load_state_dict(checkpoint['qnet_target_state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "                # 選擇性地加載學習計數\n",
    "                self.learn_step_counter = checkpoint.get('learn_step_counter', 0)\n",
    "                self.memory_counter = checkpoint.get('memory_counter', 0)\n",
    "\n",
    "                print(\"Model loaded successfully from\", file_path)\n",
    "                return {'learn_step_counter': self.learn_step_counter, 'episode': self.episode}\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(f\"No saved model found at {file_path}, starting fresh.\")\n",
    "            except KeyError as e:\n",
    "                print(f\"Error loading model: {e}\")\n",
    "\n",
    "            # 如果未成功加載模型或發生錯誤，返回初始狀態\n",
    "        return {'learn_step_counter': 0, 'episode': 0} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define Epsilon Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_compute(frame_id, epsilon_max=1, epsilon_min=0.05, epsilon_decay=100000):\n",
    "    return epsilon_min + (epsilon_max - epsilon_min) * np.exp(-frame_id / epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Output to GIF\n",
    "* remember to run it, it'll be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.getcwd()\n",
    "def save_gif(img_buffer, fname, gif_path=os.path.join(project_root, \"GIF\")):\n",
    "    if not os.path.exists(gif_path):\n",
    "        os.makedirs(gif_path)\n",
    "    img_buffer[0].save(os.path.join(gif_path, fname), save_all=True, append_images=img_buffer[1:], duration=3, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define `Play()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium_env\n",
    "def play(env, agent, stack_frames, img_size, randomized_ratio=0.005):\n",
    "    # Reset environment.\n",
    "    state = env.reset()\n",
    "    img_buffer = [Image.fromarray(state[0]*255)]\n",
    "\n",
    "    # Initialize information.\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    # One episode.\n",
    "    while True:\n",
    "        # Select action.\n",
    "        action = agent.choose_action(state, randomized_ratio)\n",
    "\n",
    "        # Get next stacked state.\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        if step % 2 == 0:\n",
    "            img_buffer.append(Image.fromarray(state_next[0]*255))\n",
    "\n",
    "        state = state_next.copy()\n",
    "        step += 1\n",
    "        total_reward += reward\n",
    "        print('\\rStep: {:3d} | Reward: {:.3f} / {:.3f} | Action: {:.3f} | Info: {}'.format(step, reward, total_reward, action, info[0]), end=\"\")\n",
    "\n",
    "        if done or step>1000:\n",
    "            print()\n",
    "            break\n",
    "\n",
    "    return img_buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(episode: int, total_steps: int, loss_arr: list, total_reward: float, episode_score: int, epsilon: float, csv_filepath: str):\n",
    " \n",
    "    # Calculate average of loss function value for each episode\n",
    "    avg_loss = np.sum(loss_arr) / len(loss_arr)\n",
    "    print(type(avg_loss))\n",
    " \n",
    "    # Store data into csv\n",
    "    df = pd.DataFrame([{\n",
    "        \"Episodes\": episode,\n",
    "        \"Total_Steps\": total_steps,\n",
    "        \"Loss\": avg_loss,\n",
    "        \"Total_Reward\": total_reward,\n",
    "        \"Score\": episode_score,\n",
    "        \"Epsilon\": epsilon,\n",
    "    }])\n",
    " \n",
    "    write_header = not os.path.exists(csv_filepath)\n",
    "    df.to_csv(csv_filepath, mode='a', header=write_header, index=False)\n",
    "    print(f\"Metrics of episode {episode} appended to {csv_filepath}!\")\n",
    " \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define `train()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, stack_frames, img_size, save_path=\"save\", max_steps=1000000, session_name=\"default\", max_episodes=10000):\n",
    "    total_step = 0\n",
    "    episode = 0\n",
    " \n",
    "    # 初始化紀錄損失值與步數\n",
    "    loss_values = []\n",
    " \n",
    " \n",
    "    # 確保保存路徑存在\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    csv_filename = f\"training_metrics_{session_name}.csv\"\n",
    "    csv_path = os.path.join(save_path, csv_filename)\n",
    "    model_filename = f\"qnet_{session_name}.pt\"\n",
    " \n",
    "    # 嘗試加載模型和訓練狀態\n",
    "    try:\n",
    "        print(\"Loading model and training status...\")\n",
    "        status = agent.save_load_model(op=\"load\", path=save_path, fname=model_filename)\n",
    "        total_step = status[\"learn_step_counter\"]\n",
    "        episode = status[\"episode\"]\n",
    "        print(f\"Resuming training from total_step={total_step}, episode={episode}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No previous model found. Starting training from scratch.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key in checkpoint: {e}\")\n",
    " \n",
    " \n",
    "    while total_step <= max_steps:\n",
    "        # Reset environment.\n",
    "        state = env.reset()\n",
    " \n",
    "        # Initialize information.\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        loss = 0\n",
    " \n",
    "        # Make sure agent episode does not exceed the limit\n",
    "        if agent.episode > max_episodes:\n",
    "            break\n",
    " \n",
    "        # One episode.\n",
    "        while True:\n",
    "            loss_values = []\n",
    "            # TODO(Lab-6): Select action.\n",
    "            epsilon = epsilon_compute(total_step)\n",
    "            action = agent.choose_action(state, epsilon)\n",
    " \n",
    "            # Get next observation.\n",
    "            obs, reward, terminated, info = env.step(action)\n",
    " \n",
    "            # 如果 obs 是 tuple，提取圖像\n",
    "            if isinstance(obs, tuple):\n",
    "                obs = obs[0]\n",
    " \n",
    "            # 判斷是否遊戲結束\n",
    "            done = terminated\n",
    " \n",
    "            # Store transition and learn.\n",
    "            agent.store_transition(state, action, reward, obs, done)\n",
    " \n",
    "            if total_step > 4 * agent.batch_size:\n",
    "                loss = agent.learn()\n",
    " \n",
    "            state = obs.copy()  # 更新狀態\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "            total_reward += reward\n",
    " \n",
    " \n",
    "            # Print status \n",
    "            if total_step % 10 == 0 or done:\n",
    "                print('\\rEpisode: {:3d} | Step: {:3d} / {:3d} | Reward: {:.3f} / {:.3f} | Loss: {:.3f} | Epsilon: {:.3f}'.format(agent.episode, step, total_step, reward, total_reward, loss, epsilon), end=\"\")\n",
    "            loss_values.append(loss)\n",
    " \n",
    "            # max step for each episode is 1000\n",
    "            # Keep track of every crucial info each episode\n",
    "            if done or step > 30:\n",
    "                write_to_csv(\n",
    "                    episode=agent.episode, \n",
    "                    total_steps=total_step, \n",
    "                    loss_arr=loss_values, \n",
    "                    total_reward=total_reward, \n",
    "                    episode_score=info[0]['total_score'], \n",
    "                    csv_filepath=csv_path,\n",
    "                    epsilon=epsilon\n",
    "                )\n",
    "            \n",
    "                agent.episode += 1\n",
    " \n",
    "            # Evaluate model for every given episode\n",
    "                if agent.episode % 20 == 0:\n",
    "                    print(\"\\nSave Model ...\")\n",
    " \n",
    "                    agent.save_load_model(\n",
    "                        op=\"save\",\n",
    "                        path=save_path,\n",
    "                        fname=model_filename\n",
    "                    )\n",
    " \n",
    " \n",
    "                    gif_name = f\"train_ep\" + str(agent.episode).zfill(5) + \".gif\"\n",
    "                    print(f\"Generate GIF <{gif_name}>...\")\n",
    "                    img_buffer = play(env, agent, stack_frames, img_size, 0.50)\n",
    "                    save_gif(img_buffer, gif_name)\n",
    "                    print(\"Done !!\")\n",
    " \n",
    "\t\t\t\t\t# Back up model\n",
    "                    if agent.episode % 400 == 0:\n",
    "                        print(\"Doing backup...\")\n",
    "                        backup_filename = f\"{model_filename}.ep{agent.episode}.qnet.bak\"\n",
    "                        orig_path = os.path.join(save_path, model_filename)\n",
    "                        backup_path = os.path.join(save_path, backup_filename)\n",
    "                        shutil.copy(orig_path, backup_path)\n",
    "                        print(f\"Backup done, file path: {backup_path}\")\n",
    " \n",
    "                \n",
    "                break\n",
    " \n",
    " \n",
    "            if total_step > max_steps:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 600step/min on Colab (with T4 GPU), 400step/min on RTX3070 laptop. Pretty slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'gymnasium_env/PacmanGymEnv'\n",
    "env = gym.make(env_name, speedup=4.0)\n",
    "env_pacman = PacmanEnvWrapper(env=env, k=4, img_size=(84, 84))\n",
    "stack_frames = 4\n",
    "img_size = (84,84)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "agent = DeepQNetwork(\n",
    "        n_actions = env.action_space.n,\n",
    "        input_shape = [stack_frames, *img_size],\n",
    "        qnet = QNet,\n",
    "        device = device,\n",
    "        learning_rate = 2e-5,\n",
    "        reward_decay = 0.95,\n",
    "        replace_target_iter = 1000,\n",
    "        memory_size = 10000,\n",
    "        batch_size = 32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env_pacman, agent, stack_frames, img_size, save_path=os.path.join(project_root, \"save\"), max_steps=600000, session_name=\"DDQN_NoGrid_Advanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_load_model(op=\"load\", path=os.path.join(project_root, \"save\"), fname=\"qnet.pt\")\n",
    "env_pacman = PacmanEnvWrapper(env, k=4, img_size=(84,84))\n",
    "img_buffer = play(env_pacman, agent, stack_frames, img_size)\n",
    "save_gif(img_buffer, \"eval.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pacman-RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
